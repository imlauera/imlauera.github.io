<!DOCTYPE html>
<html lang="es">





<head>
  <meta property="og:url" content="https://imlauera.github.io/cheap_llm/">
  <meta property="og:site_name" content="Imlauer">
  <meta property="og:title" content="LLM desde la terminal gratis (Groq)">
  <meta property="og:description" content="Usa aichat tenes que crearte una cuenta en https://console.groq.com/ y una api key.
Opción 1: LLMs locales ligeros (RECOMENDADO) Aunque digas que tu hardware es ligero, hay modelos que funcionan incluso en CPUs modestas:
Ollama con modelos pequeños:
# Instalar desde AUR yay -S ollama # Iniciar servicio sudo systemctl start ollama # Modelos muy ligeros (1.5-3GB RAM): ollama run llama3.2:1b # El más ligero, 1GB ollama run phi3:mini # 2.3GB ollama run qwen2.5:1.5b # 1.5GB Luego puedes usarlo desde consola:">
  <meta property="og:locale" content="es_es">
  <meta property="og:type" content="article">
    <meta property="article:published_time" content="2026-02-11T04:00:53-03:00">
    <meta property="article:modified_time" content="2026-02-11T04:00:53-03:00">
    <meta property="article:tag" content="Llm">

  
  <meta itemprop="name" content="LLM desde la terminal gratis (Groq)">
  <meta itemprop="description" content="Usa aichat tenes que crearte una cuenta en https://console.groq.com/ y una api key.
Opción 1: LLMs locales ligeros (RECOMENDADO) Aunque digas que tu hardware es ligero, hay modelos que funcionan incluso en CPUs modestas:
Ollama con modelos pequeños:
# Instalar desde AUR yay -S ollama # Iniciar servicio sudo systemctl start ollama # Modelos muy ligeros (1.5-3GB RAM): ollama run llama3.2:1b # El más ligero, 1GB ollama run phi3:mini # 2.3GB ollama run qwen2.5:1.5b # 1.5GB Luego puedes usarlo desde consola:">
  <meta itemprop="datePublished" content="2026-02-11T04:00:53-03:00">
  <meta itemprop="dateModified" content="2026-02-11T04:00:53-03:00">
  <meta itemprop="wordCount" content="364">
  <meta itemprop="keywords" content="Llm">
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="LLM desde la terminal gratis (Groq)">
  <meta name="twitter:description" content="Usa aichat tenes que crearte una cuenta en https://console.groq.com/ y una api key.
Opción 1: LLMs locales ligeros (RECOMENDADO) Aunque digas que tu hardware es ligero, hay modelos que funcionan incluso en CPUs modestas:
Ollama con modelos pequeños:
# Instalar desde AUR yay -S ollama # Iniciar servicio sudo systemctl start ollama # Modelos muy ligeros (1.5-3GB RAM): ollama run llama3.2:1b # El más ligero, 1GB ollama run phi3:mini # 2.3GB ollama run qwen2.5:1.5b # 1.5GB Luego puedes usarlo desde consola:">

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>
    
    LLM desde la terminal gratis (Groq)
    
  </title>
  <link rel="stylesheet" href='https://imlauera.github.io/css/site.min.css'>
  <link rel="canonical" href="https://imlauera.github.io/cheap_llm/">
  <link rel="alternate" type="application/rss&#43;xml" href="https://imlauera.github.io/index.xml" title="Imlauer">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="referrer" content="no-referrer">
  <meta name="author" content="Imlauer.">
  <meta name="description" content="Usa aichat tenes que crearte una cuenta en https://console.groq.com/ y una api key.
Opción 1: LLMs locales ligeros (RECOMENDADO)
Aunque digas que tu hardware es ligero, hay modelos que funcionan incluso en CPUs modestas:
Ollama con modelos pequeños:
# Instalar desde AUR
yay -S ollama

# Iniciar servicio
sudo systemctl start ollama

# Modelos muy ligeros (1.5-3GB RAM):
ollama run llama3.2:1b        # El más ligero, 1GB
ollama run phi3:mini          # 2.3GB
ollama run qwen2.5:1.5b       # 1.5GB
Luego puedes usarlo desde consola:">
</head>
<body><nav class="navbar is-transparent " role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a class="navbar-item" href="https://imlauera.github.io/">
      <figure class="image">
        <img alt="" class="is-rounded" src="/img/memememe.jpg">
      </figure>
    </a>
    <a class="navbar-item" href="https://imlauera.github.io/">
      Imlauer
    </a>
    <a class="navbar-item" href="/acerca/">
      Acerca de Mi
    </a>
  </div>
  
  
</nav>

  <section>
    <section class='hero is-small is-link is-fullwidth'>
      <div class="hero-body">
<div class="container">
  <h1 class="title">
    LLM desde la terminal gratis (Groq)
  </h1>
  <h2 class="subtitle">
    <time datetime='2026-02-11T04:00:53-03:00'>
      February 11, 2026
    </time>
    
    <br>
    
    
    
    <a class="tag is-info" href="https://imlauera.github.io/tags/llm/">Llm</a>
    
    
    
  </h2>
</div>

      </div>
    </section>
    <section class="section">
      <div class="container">
<div class="content is-medium">
  <p>Usa aichat tenes que crearte una cuenta en <a href="https://console.groq.com/">https://console.groq.com/</a> y una api key.</p>
<h2 id="opción-1-llms-locales-ligeros-recomendado">Opción 1: LLMs locales ligeros (RECOMENDADO)</h2>
<p>Aunque digas que tu hardware es ligero, hay modelos que funcionan incluso en CPUs modestas:</p>
<p><strong>Ollama</strong> con modelos pequeños:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Instalar desde AUR</span>
</span></span><span style="display:flex;"><span>yay -S ollama
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Iniciar servicio</span>
</span></span><span style="display:flex;"><span>sudo systemctl start ollama
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Modelos muy ligeros (1.5-3GB RAM):</span>
</span></span><span style="display:flex;"><span>ollama run llama3.2:1b        <span style="color:#75715e"># El más ligero, 1GB</span>
</span></span><span style="display:flex;"><span>ollama run phi3:mini          <span style="color:#75715e"># 2.3GB</span>
</span></span><span style="display:flex;"><span>ollama run qwen2.5:1.5b       <span style="color:#75715e"># 1.5GB</span>
</span></span></code></pre></div><p>Luego puedes usarlo desde consola:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama run llama3.2:1b <span style="color:#e6db74">&#34;explícame qué hace grep&#34;</span>
</span></span></code></pre></div><h2 id="opción-2-apis-gratuitas">Opción 2: APIs gratuitas</h2>
<p>Hay APIs LLM gratuitas sin pagar:</p>
<p><strong>Groq</strong> (muy rápido, gratis):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Registrarte en groq.com para API key</span>
</span></span><span style="display:flex;"><span>export GROQ_API_KEY<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tu_key&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>curl -X POST <span style="color:#e6db74">&#34;https://api.groq.com/openai/v1/chat/completions&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>  -H <span style="color:#e6db74">&#34;Authorization: Bearer </span>$GROQ_API_KEY<span style="color:#e6db74">&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>  -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>  -d <span style="color:#e6db74">&#39;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;model&#34;: &#34;llama-3.3-70b-versatile&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;tu pregunta&#34;}]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }&#39;</span>
</span></span></code></pre></div><h2 id="opción-3-tu-idea-de-scraping-menos-recomendado">Opción 3: Tu idea de scraping (menos recomendado)</h2>
<p>Si insistes en scraping de SearX:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e">#!/bin/bash
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"># buscar.sh</span>
</span></span><span style="display:flex;"><span>QUERY<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span>$*<span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>curl -s <span style="color:#e6db74">&#34;http://localhost:8888/search?q=</span><span style="color:#e6db74">${</span>QUERY// /+<span style="color:#e6db74">}</span><span style="color:#e6db74">&amp;format=json&#34;</span> | <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>  jq -r <span style="color:#e6db74">&#39;.results[] | .title + &#34;\n&#34; + .content + &#34;\n&#34; + .url + &#34;\n---&#34;&#39;</span> &gt; /tmp/resultados.txt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>grep -i <span style="color:#e6db74">&#34;</span>$1<span style="color:#e6db74">&#34;</span> /tmp/resultados.txt
</span></span></code></pre></div><p><strong>¿Qué hardware tienes?</strong> Con 4GB RAM ya puedes correr modelos de 1-3B parámetros que son sorprendentemente capaces para uso en terminal.</p>
<p>Sí, hay varias opciones de CLI para Groq:</p>
<h2 id="1-groq-cli-oficial-en-npm">1. <strong>groq-cli</strong> (oficial en npm)</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Instalar</span>
</span></span><span style="display:flex;"><span>npm install -g groq-cli
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># O con pnpm/yarn</span>
</span></span><span style="display:flex;"><span>pnpm install -g groq-cli
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Configurar</span>
</span></span><span style="display:flex;"><span>export GROQ_API_KEY<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tu_key&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Usar</span>
</span></span><span style="display:flex;"><span>groq <span style="color:#e6db74">&#34;tu pregunta aquí&#34;</span>
</span></span></code></pre></div><h2 id="2-llm-de-simon-willison-más-versátil">2. <strong>llm</strong> de Simon Willison (más versátil)</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Instalar (Python)</span>
</span></span><span style="display:flex;"><span>pipx install llm
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Instalar plugin de Groq</span>
</span></span><span style="display:flex;"><span>llm install llm-groq
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Configurar</span>
</span></span><span style="display:flex;"><span>llm keys set groq
</span></span><span style="display:flex;"><span><span style="color:#75715e"># (pega tu API key)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Usar</span>
</span></span><span style="display:flex;"><span>llm -m groq/llama-3.3-70b-versatile <span style="color:#e6db74">&#34;tu pregunta&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Alias para más comodidad</span>
</span></span><span style="display:flex;"><span>alias ask<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;llm -m groq/llama-3.3-70b-versatile&#39;</span>
</span></span><span style="display:flex;"><span>ask <span style="color:#e6db74">&#34;cómo funciona grep&#34;</span>
</span></span></code></pre></div><h2 id="3-script-bash-simple-sin-dependencias">3. <strong>Script bash simple</strong> (sin dependencias)</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e">#!/bin/bash
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"># ~/bin/groq</span>
</span></span><span style="display:flex;"><span>GROQ_API_KEY<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;tu_key_aqui&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>curl -s https://api.groq.com/openai/v1/chat/completions <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>  -H <span style="color:#e6db74">&#34;Authorization: Bearer </span>$GROQ_API_KEY<span style="color:#e6db74">&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>  -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span>  -d <span style="color:#e6db74">&#34;{
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    \&#34;model\&#34;: \&#34;llama-3.3-70b-versatile\&#34;,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    \&#34;messages\&#34;: [{\&#34;role\&#34;: \&#34;user\&#34;, \&#34;content\&#34;: \&#34;</span>$*<span style="color:#e6db74">\&#34;}]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">  }&#34;</span> | jq -r <span style="color:#e6db74">&#39;.choices[0].message.content&#39;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>chmod +x ~/bin/groq
</span></span><span style="display:flex;"><span>groq <span style="color:#e6db74">&#34;explícame systemd&#34;</span>
</span></span></code></pre></div><p><strong>Mi recomendación:</strong> <code>llm</code> de Simon Willison porque:</p>
<ul>
<li>Soporta múltiples proveedores (Groq, OpenAI, Claude, Ollama local)</li>
<li>Muy activo y mantenido</li>
<li>Historial de conversaciones</li>
<li>Fácil cambiar entre modelos</li>
</ul>
<p>¿Cuál prefieres probar primero?</p>

</div>


      </div>
    </section>
  </section><script 
src="/quiz/quizdown.js">
</script>
<script 
src="/quiz/quizdownKatex.js">
</script>
<script 
src="/quiz/quizdownHighlight.js">
</script>
<script>quizdown.register(quizdownHighlight).register(quizdownKatex).init()</script> 
<footer class="footer">
  <div class="content has-text-centered">
    
    
    <p>
      
      <a class="" href="https://imlauera.github.io/index.xml" target="_blank">
        <span>
          RSS
        </span>
      </a>
      
      | <a href="https://imlauera.github.io" target="_blank">Andres Imlauer.</a> 

      
      
    </p>
    
  </div>




<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} })
</script>
</footer>


</body>
</html>
