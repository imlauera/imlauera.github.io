<!DOCTYPE html>
<html lang="es">





<head>
  <meta property="og:url" content="https://imlauera.github.io/descargar_websites_con_wget/">
  <meta property="og:site_name" content="Imlauer">
  <meta property="og:title" content="Como usar wget y WikiTeam3 para descargar un sitio MediaWiki y convertirlos a ZIM para leerlos con Kiwix.">
  <meta property="og:description" content="Nunca mires nada online. Siempre descargatelo y si es mejor usa Tor. Anteriormente a esto me descargu√© Kiwix, los docs de ArchLinux, Gentoo y la inciclopedia.
wget --mirror --convert-links --adjust-extension --page-requisites --no-parent https://geohot.github.io/blog/ wget --mirror --convert-links --adjust-extension --page-requisites --no-parent --recursive --level=5 --span-hosts --domains=harmful.cat-v.org http://harmful.cat-v.org/ Si el sitio te bloquea: wget -e robots=off --user-agent=&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.37&#34; --mirror --convert-links --adjust-extension --page-requisites --no-parent --recursive --level=5 --span-hosts --domains=harmful.cat-v.org http://harmful.cat-v.org/ üîç ¬øQu√© hace cada opci√≥n? Opci√≥n Funci√≥n --mirror Activa recursividad, respeta tiempos, etc. (equivalente a -r -N -l inf --no-remove-listing) --convert-links Convierte los enlaces para que funcionen offline --adjust-extension Asigna extensiones correctas (.html) --page-requisites Descarga CSS, im√°genes y JS necesarios --no-parent Evita subir a directorios superiores URL P√°gina inicial del sitio a copiar üìå Aumentar profundidad (descargar m√°s p√°ginas) wget -r -l 5 --convert-links --page-requisites --adjust-extension --no-parent https://incels.wiki/ -l 5 significa ‚Äúcinco niveles de recursi√≥n‚Äù. Puedes aumentar, pero es m√°s lento.">
  <meta property="og:locale" content="es_es">
  <meta property="og:type" content="article">
    <meta property="article:published_time" content="2025-11-23T04:34:21-03:00">
    <meta property="article:modified_time" content="2025-11-23T04:34:21-03:00">
    <meta property="article:tag" content="Cli">

  
  <meta itemprop="name" content="Como usar wget y WikiTeam3 para descargar un sitio MediaWiki y convertirlos a ZIM para leerlos con Kiwix.">
  <meta itemprop="description" content="Nunca mires nada online. Siempre descargatelo y si es mejor usa Tor. Anteriormente a esto me descargu√© Kiwix, los docs de ArchLinux, Gentoo y la inciclopedia.
wget --mirror --convert-links --adjust-extension --page-requisites --no-parent https://geohot.github.io/blog/ wget --mirror --convert-links --adjust-extension --page-requisites --no-parent --recursive --level=5 --span-hosts --domains=harmful.cat-v.org http://harmful.cat-v.org/ Si el sitio te bloquea: wget -e robots=off --user-agent=&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.37&#34; --mirror --convert-links --adjust-extension --page-requisites --no-parent --recursive --level=5 --span-hosts --domains=harmful.cat-v.org http://harmful.cat-v.org/ üîç ¬øQu√© hace cada opci√≥n? Opci√≥n Funci√≥n --mirror Activa recursividad, respeta tiempos, etc. (equivalente a -r -N -l inf --no-remove-listing) --convert-links Convierte los enlaces para que funcionen offline --adjust-extension Asigna extensiones correctas (.html) --page-requisites Descarga CSS, im√°genes y JS necesarios --no-parent Evita subir a directorios superiores URL P√°gina inicial del sitio a copiar üìå Aumentar profundidad (descargar m√°s p√°ginas) wget -r -l 5 --convert-links --page-requisites --adjust-extension --no-parent https://incels.wiki/ -l 5 significa ‚Äúcinco niveles de recursi√≥n‚Äù. Puedes aumentar, pero es m√°s lento.">
  <meta itemprop="datePublished" content="2025-11-23T04:34:21-03:00">
  <meta itemprop="dateModified" content="2025-11-23T04:34:21-03:00">
  <meta itemprop="wordCount" content="951">
  <meta itemprop="keywords" content="Cli">
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Como usar wget y WikiTeam3 para descargar un sitio MediaWiki y convertirlos a ZIM para leerlos con Kiwix.">
  <meta name="twitter:description" content="Nunca mires nada online. Siempre descargatelo y si es mejor usa Tor. Anteriormente a esto me descargu√© Kiwix, los docs de ArchLinux, Gentoo y la inciclopedia.
wget --mirror --convert-links --adjust-extension --page-requisites --no-parent https://geohot.github.io/blog/ wget --mirror --convert-links --adjust-extension --page-requisites --no-parent --recursive --level=5 --span-hosts --domains=harmful.cat-v.org http://harmful.cat-v.org/ Si el sitio te bloquea: wget -e robots=off --user-agent=&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.37&#34; --mirror --convert-links --adjust-extension --page-requisites --no-parent --recursive --level=5 --span-hosts --domains=harmful.cat-v.org http://harmful.cat-v.org/ üîç ¬øQu√© hace cada opci√≥n? Opci√≥n Funci√≥n --mirror Activa recursividad, respeta tiempos, etc. (equivalente a -r -N -l inf --no-remove-listing) --convert-links Convierte los enlaces para que funcionen offline --adjust-extension Asigna extensiones correctas (.html) --page-requisites Descarga CSS, im√°genes y JS necesarios --no-parent Evita subir a directorios superiores URL P√°gina inicial del sitio a copiar üìå Aumentar profundidad (descargar m√°s p√°ginas) wget -r -l 5 --convert-links --page-requisites --adjust-extension --no-parent https://incels.wiki/ -l 5 significa ‚Äúcinco niveles de recursi√≥n‚Äù. Puedes aumentar, pero es m√°s lento.">

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>
    
    Como usar wget y WikiTeam3 para descargar un sitio MediaWiki y convertirlos a ZIM para leerlos con Kiwix.
    
  </title>
  <link rel="stylesheet" href='https://imlauera.github.io/css/site.min.css'>
  <link rel="canonical" href="https://imlauera.github.io/descargar_websites_con_wget/">
  <link rel="alternate" type="application/rss&#43;xml" href="https://imlauera.github.io/index.xml" title="Imlauer">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="referrer" content="no-referrer">
  <meta name="author" content="Imlauer.">
  <meta name="description" content="Nunca mires nada online. Siempre descargatelo y si es mejor usa Tor.
Anteriormente a esto me descargu√© Kiwix, los docs de ArchLinux, Gentoo y la inciclopedia.
wget --mirror --convert-links --adjust-extension --page-requisites --no-parent https://geohot.github.io/blog/

wget --mirror --convert-links --adjust-extension --page-requisites --no-parent --recursive --level=5 --span-hosts --domains=harmful.cat-v.org http://harmful.cat-v.org/

Si el sitio te bloquea:

wget -e robots=off --user-agent=&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.37&#34; --mirror --convert-links --adjust-extension --page-requisites --no-parent --recursive --level=5 --span-hosts --domains=harmful.cat-v.org http://harmful.cat-v.org/
üîç ¬øQu√© hace cada opci√≥n?

  
      
          Opci√≥n
          Funci√≥n
      
  
  
      
          --mirror
          Activa recursividad, respeta tiempos, etc. (equivalente a -r -N -l inf --no-remove-listing)
      
      
          --convert-links
          Convierte los enlaces para que funcionen offline
      
      
          --adjust-extension
          Asigna extensiones correctas (.html)
      
      
          --page-requisites
          Descarga CSS, im√°genes y JS necesarios
      
      
          --no-parent
          Evita subir a directorios superiores
      
      
          URL
          P√°gina inicial del sitio a copiar
      
  


üìå Aumentar profundidad (descargar m√°s p√°ginas)
wget -r -l 5 --convert-links --page-requisites --adjust-extension --no-parent https://incels.wiki/
-l 5 significa ‚Äúcinco niveles de recursi√≥n‚Äù. Puedes aumentar, pero es m√°s lento.">
</head>
<body><nav class="navbar is-transparent " role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a class="navbar-item" href="https://imlauera.github.io/">
      <figure class="image">
        <img alt="" class="is-rounded" src="/img/memememe.jpg">
      </figure>
    </a>
    <a class="navbar-item" href="https://imlauera.github.io/">
      Imlauer
    </a>
    <a class="navbar-item" href="/acerca/">
      Acerca de Mi
    </a>
  </div>
  
  
</nav>

  <section>
    <section class='hero is-small is-link is-fullwidth'>
      <div class="hero-body">
<div class="container">
  <h1 class="title">
    Como usar wget y WikiTeam3 para descargar un sitio MediaWiki y convertirlos a ZIM para leerlos con Kiwix.
  </h1>
  <h2 class="subtitle">
    <time datetime='2025-11-23T04:34:21-03:00'>
      November 23, 2025
    </time>
    
    <br>
    
    
    
    <a class="tag is-info" href="https://imlauera.github.io/tags/cli/">Cli</a>
    
    
    
  </h2>
</div>

      </div>
    </section>
    <section class="section">
      <div class="container">
<div class="content is-medium">
  <p>Nunca mires nada online. Siempre descargatelo y si es mejor usa Tor.
Anteriormente a esto me descargu√© Kiwix, los docs de ArchLinux, Gentoo y la inciclopedia.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget --mirror --convert-links --adjust-extension --page-requisites --no-parent https://geohot.github.io/blog/
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>wget --mirror --convert-links --adjust-extension --page-requisites --no-parent --recursive --level<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span> --span-hosts --domains<span style="color:#f92672">=</span>harmful.cat-v.org http://harmful.cat-v.org/
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Si el sitio te bloquea:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>wget -e robots<span style="color:#f92672">=</span>off --user-agent<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.37&#34;</span> --mirror --convert-links --adjust-extension --page-requisites --no-parent --recursive --level<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span> --span-hosts --domains<span style="color:#f92672">=</span>harmful.cat-v.org http://harmful.cat-v.org/
</span></span></code></pre></div><h3 id="-qu√©-hace-cada-opci√≥n">üîç ¬øQu√© hace cada opci√≥n?</h3>
<table>
  <thead>
      <tr>
          <th>Opci√≥n</th>
          <th>Funci√≥n</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>--mirror</code></td>
          <td>Activa recursividad, respeta tiempos, etc. (equivalente a <code>-r -N -l inf --no-remove-listing</code>)</td>
      </tr>
      <tr>
          <td><code>--convert-links</code></td>
          <td>Convierte los enlaces para que funcionen offline</td>
      </tr>
      <tr>
          <td><code>--adjust-extension</code></td>
          <td>Asigna extensiones correctas (.html)</td>
      </tr>
      <tr>
          <td><code>--page-requisites</code></td>
          <td>Descarga CSS, im√°genes y JS necesarios</td>
      </tr>
      <tr>
          <td><code>--no-parent</code></td>
          <td>Evita subir a directorios superiores</td>
      </tr>
      <tr>
          <td>URL</td>
          <td>P√°gina inicial del sitio a copiar</td>
      </tr>
  </tbody>
</table>
<hr>
<h1 id="-aumentar-profundidad-descargar-m√°s-p√°ginas">üìå Aumentar profundidad (descargar m√°s p√°ginas)</h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget -r -l <span style="color:#ae81ff">5</span> --convert-links --page-requisites --adjust-extension --no-parent https://incels.wiki/
</span></span></code></pre></div><p><code>-l 5</code> significa ‚Äúcinco niveles de recursi√≥n‚Äù. Puedes aumentar, pero es m√°s lento.</p>
<hr>
<h1 id="-si-el-sitio-bloquea-wget">üìå Si el sitio bloquea <code>wget</code></h1>
<p>Algunas wikis bloquean agentes autom√°ticos. Puedes intentar:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget --mirror --convert-links --page-requisites --adjust-extension --no-parent --user-agent<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Mozilla/5.0&#34;</span> https://incels.wiki/
</span></span></code></pre></div><ul>
<li><strong>Respeta la licencia del contenido</strong> (muchas wikis usan CC-BY-SA).</li>
</ul>
<hr>
<h3 id="hacer-un-dump-de-xml-con-wikiteam3">Hacer un dump de XML con WikiTeam3</h3>
<p>Muy buena pregunta. Si quer√©s un <strong>dump XML</strong> de incels.wiki (o ‚Äúincel.wiki‚Äù), hay algunas limitaciones importantes, m√°s que nada si no sos administrador del sitio. No encontr√© evidencia p√∫blica de que incels.wiki tenga dumps oficiales disponibles para descarga libre, por lo que las opciones son m√°s bien ‚Äúhacer tu propio dump‚Äù mediante herramientas o pedirlo a los due√±os del wiki.</p>
<hr>
<h2 id="-opciones-para-hacer-un-dump-xml">‚öôÔ∏è Opciones para hacer un dump XML</h2>
<ol>
<li>
<p><strong>Usar WikiTeam3</strong></p>
<ul>
<li>
<p>WikiTeam tiene una herramienta llamada <strong>WikiTeam3 / MediaWiki Dump Generator</strong>, que permite generar un dump XML de la wiki, usando la API o el scraping. ([Archive Team][1])</p>
</li>
<li>
<p>Esta herramienta puede guardar <strong>el historial completo</strong> o solo las revisiones m√°s recientes. ([Archive Team][1])</p>
</li>
<li>
<p>Ejemplo de comando (suponiendo que us√°s Python 3):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wikiteam3dumpgenerator https://incels.wiki --xml --xmlrevisions
</span></span></code></pre></div></li>
<li>
<p>Esto va a hacer llamadas a la API de MediaWiki para bajar los art√≠culos y sus revisiones (no es exactamente igual a usar <code>dumpBackup.php</code>, pero es lo m√°s cercano si no ten√©s acceso al servidor).</p>
</li>
</ul>
</li>
<li>
<p><strong>Si tuvieras acceso al servidor</strong></p>
<ul>
<li>
<p>Si vos o alguien que conozcas administra esa instancia de MediaWiki, pod√©s usar el script <code>dumpBackup.php</code> que viene con MediaWiki para volcar toda la wiki en XML. ([MediaWiki][2])</p>
</li>
<li>
<p>Ejemplo:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>php maintenance/run.php dumpBackup --full &gt; dump_completo.xml
</span></span></code></pre></div></li>
<li>
<p>Opcionalmente, incluir archivos subidos con <code>--include-files --uploads</code>. ([MediaWiki][2])</p>
</li>
<li>
<p>Este m√©todo es el m√°s ‚Äúlimpio‚Äù, porque genera un volcado estructurado con todas las p√°ginas y sus revisiones.</p>
</li>
</ul>
</li>
</ol>
<hr>
<h2 id="-qu√©-hacer-si-no-hay-un-dump-p√∫blico">‚úÖ Qu√© hacer si no hay un dump p√∫blico</h2>
<ul>
<li>Pod√©s <strong>preguntar a los administradores</strong> de incels.wiki si tienen un dump XML para compartir.</li>
<li>Si no te responden, la opci√≥n con WikiTeam3 es probablemente la m√°s ‚Äúrespetuosa‚Äù y t√©cnica para generar un dump por tu cuenta, usando la API p√∫blica.</li>
</ul>
<hr>
<p>Si quer√©s, puedo chequear <strong>en Internet Archive</strong> o en otros repositorios para ver si hay ya un dump de incels.wiki disponible p√∫blicamente, y te comparto el enlace si existe. ¬øTe parece bien hacer eso?</p>
<hr>
<p>Buena idea. Hice una b√∫squeda, pero no encontr√© un <strong>dump p√∫blico en Internet Archive</strong> para <strong>incels.wiki</strong>: no hay registros obvios en la Wayback Machine ni un archivo completo descargable desde Archive.org con todas las p√°ginas del wiki.</p>
<p>Sin embargo, s√≠ encontr√© algo relevante:</p>
<ul>
<li>En <strong>Hugging Face</strong> hay un dataset llamado <em>‚Äúincelwiki‚Äë20240225‚Äëdump‚Äù</em>, que parece ser un volcado de la wiki hasta el <strong>25 de febrero de 2024</strong>. ([Hugging Face][1])</li>
<li>En dicho dataset est√°n los textos m√°s recientes (‚Äútext‚Äù: el contenido de la revisi√≥n m√°s reciente) y un historial de revisiones (‚Äúrevisions‚Äù) por p√°gina. ([Hugging Face][1])</li>
<li>Tambi√©n hay un archivo <code>siteinfo.json</code> que describe la estructura del sitio original (nombre de la web, base URL, colaci√≥n de categor√≠as, etc.). ([Hugging Face][2])</li>
</ul>
<p><a href="https://huggingface.co/api/resolve-cache/datasets/NyxKrage/incelwiki-20240225-dump/6b5514611ff193832ce1d6809b9ff6201f99224d/incelwiki-20240225-dump.py?download=true&amp;utm_source=chatgpt.com&amp;etag=%22cff77a827e280a84ea4e1304bba965836b33328f%22">https://huggingface.co/api/resolve-cache/datasets/NyxKrage/incelwiki-20240225-dump/6b5514611ff193832ce1d6809b9ff6201f99224d/incelwiki-20240225-dump.py?download=true&amp;utm_source=chatgpt.com&amp;etag=%22cff77a827e280a84ea4e1304bba965836b33328f%22</a></p>
<hr>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget --mirror --convert-links --adjust-extension --page-requisites --no-parent --tries<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>  https://stallman.org
</span></span></code></pre></div><hr>
<p>El comando anterior era muy profundo as√≠ que us√© este:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget -r -l <span style="color:#ae81ff">5</span> --convert-links --adjust-extension --page-requisites --no-parent --tries<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span> https://stallman.org
</span></span></code></pre></div><hr>
<p>Short answer: <strong>Yes, you <em>can</em> use <code>wget</code> to download content and then create your own local ‚Äúweb‚Äù (offline site) for use with Kiwix ‚Äî but only under certain conditions.</strong>
However, it‚Äôs important to understand how Kiwix works so you do it the correct way.</p>
<hr>
<h2 id="-how-kiwix-actually-works">‚úÖ How Kiwix Actually Works</h2>
<p>Kiwix does <strong>not</strong> use regular HTML website folders.
Instead, it uses <strong>ZIM files</strong>, a special highly‚Äìcompressed offline web format.</p>
<p>So if you want to ‚Äúcreate a web for Kiwix,‚Äù you need to:</p>
<ol>
<li><strong>Download or prepare content</strong></li>
<li><strong>Convert it into a ZIM file</strong></li>
<li><strong>Host or load that ZIM file inside Kiwix</strong></li>
</ol>
<hr>
<h2 id="-option-1--download-existing-zim-files-recommended">‚úîÔ∏è Option 1 ‚Äî Download existing ZIM files (recommended)</h2>
<p>You <strong>do not need <code>wget</code></strong> for this unless you want automated downloading.</p>
<p>Kiwix library:
<a href="https://library.kiwix.org/">https://library.kiwix.org/</a> (do not open from ChatGPT)</p>
<p>Download ZIM files such as:</p>
<ul>
<li>Wikipedia</li>
<li>Wikivoyage</li>
<li>Wikibooks</li>
<li>StackExchange dumps</li>
<li>TED Talks</li>
<li>Gutenberg Library</li>
</ul>
<p>You can script downloading the ZIM files using <code>wget</code> like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget -c https://download.kiwix.org/zim/wikipedia_en_all_nopic.zim
</span></span></code></pre></div><hr>
<h2 id="-option-2--use-wget-to-download-a-website-and-then-convert-it-into-zim">‚úîÔ∏è Option 2 ‚Äî Use <code>wget</code> to download a website and then convert it into ZIM</h2>
<p>If you want your <strong>own website</strong> in Kiwix, you can capture it with <code>wget</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget --mirror --convert-links --adjust-extension --page-requisites --no-parent https://example.com/
</span></span></code></pre></div><p>This gives you a local copy of a site.</p>
<p>Then convert it to a ZIM file:</p>
<h3 id="install-zimwriterfs">Install <code>zimwriterfs</code></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt install zimwriterfs
</span></span></code></pre></div><h3 id="create-the-zim">Create the ZIM</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>zimwriterfs ./example.com example.zim
</span></span></code></pre></div><p>Now you can open <code>example.zim</code> inside Kiwix.</p>
<hr>
<h2 id="-option-3--build-your-own-web-library-for-kiwix-server">‚úîÔ∏è Option 3 ‚Äî Build your own ‚Äúweb library‚Äù for Kiwix Server</h2>
<p>Once you have several ZIM files, you can run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kiwix-serve --port<span style="color:#f92672">=</span><span style="color:#ae81ff">8080</span> /path/to/your/library
</span></span></code></pre></div><p>This creates a <strong>local offline web</strong> containing all your ZIM files, accessible through:</p>
<pre tabindex="0"><code>http://localhost:8080
</code></pre><hr>
<h2 id="-want-me-to-generate-a-full-script-that">‚ùì Want me to generate a full script that:</h2>
<ul>
<li>Downloads a site with <code>wget</code></li>
<li>Converts it to ZIM</li>
<li>Automatically adds it to a Kiwix server?</li>
</ul>
<p>Just tell me your OS (Linux, Windows, macOS) and the site you want to capture.</p>

</div>


      </div>
    </section>
  </section><script 
src="/quiz/quizdown.js">
</script>
<script 
src="/quiz/quizdownKatex.js">
</script>
<script 
src="/quiz/quizdownHighlight.js">
</script>
<script>quizdown.register(quizdownHighlight).register(quizdownKatex).init()</script> 
<footer class="footer">
  <div class="content has-text-centered">
    
    
    <p>
      
      <a class="" href="https://imlauera.github.io/index.xml" target="_blank">
        <span>
          RSS
        </span>
      </a>
      
      | <a href="https://imlauera.github.io" target="_blank">Andres Imlauer.</a> 

      
      
    </p>
    
  </div>




<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} })
</script>
</footer>


</body>
</html>
